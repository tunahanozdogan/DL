{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30ZRqlMF9Ye2"
   },
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9bC-87o9Ye9"
   },
   "source": [
    "<h1 style=\"text-align: center;\">Deep Learning<br><br>Session - 4<br><br>Classification with ANN<br><br>Cancer Data<br><h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tensorflow playground:***\n",
    "https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lwwib4r9Ye-"
   },
   "source": [
    "# Keras Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6VD4eWBQcze2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set it None to display all rows in the dataframe\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Set it to None to display all columns in the dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KD58Xyty9YfC"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNxA5KEQ9YfC"
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jQaMO7D9YfD",
    "outputId": "eea56130-020c-4eb1-e582-c85b15543bd0"
   },
   "outputs": [],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsFZ3gLD9YfF",
    "outputId": "e59832c4-a31a-4315-d56e-4b5f5dcd9043"
   },
   "outputs": [],
   "source": [
    "print(cancer[\"target_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6cCGWJU9YfG",
    "outputId": "0ad8255d-551d-4282-a5ca-e79ca28f8598"
   },
   "outputs": [],
   "source": [
    "print(cancer[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_KCEmx29YfG",
    "outputId": "7116c247-a512-41ee-a0ec-01c2094ce022"
   },
   "outputs": [],
   "source": [
    "cancer[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "us3ZsV7D9YfH",
    "outputId": "e543b693-69e1-488b-e01e-74319a3dfd3a"
   },
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPQvgHox9YfH",
    "outputId": "acc285ad-c3b2-4ee9-cc62-72d2cd1c0850"
   },
   "outputs": [],
   "source": [
    "df_target = pd.DataFrame(cancer['target'],columns=['Cancer'])\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZoB208D9YfI",
    "outputId": "028901e7-dba4-4a4e-f7df-cd1cb4f69cb6"
   },
   "outputs": [],
   "source": [
    "frames = [df_feat, df_target]\n",
    "df = pd.concat(frames,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "719Ia9Nv9YfI"
   },
   "source": [
    "## Exploratory Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPyMUnVWfM0o",
    "outputId": "7a294ad3-49d9-4cea-b6a2-9824e2c0ba4a"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ROCQa9wf2uE",
    "outputId": "40e94921-6a7c-48c6-8c09-2623b6b60fa3"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mdUkgZD-gCUx",
    "outputId": "cd4f6662-e8c3-4a6f-93d5-26fed83d8a82"
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4wc-w4c9YfJ",
    "outputId": "77b88970-203a-480a-e967-0f9756769fd5"
   },
   "outputs": [],
   "source": [
    "display(df.Cancer.value_counts())\n",
    "ax = sns.countplot(x=df[\"Cancer\"])\n",
    "ax.bar_label(ax.containers[0], size=16)\n",
    "plt.axhline(y=df.Cancer.value_counts()[1], color='orange', linestyle='--')\n",
    "plt.axhline(y=df.Cancer.value_counts()[0], color='blue', linestyle='--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"box\",\n",
    "        by=\"Cancer\", \n",
    "        layout=(5, 6), \n",
    "        subplots=True, \n",
    "        widths=0.7, \n",
    "        figsize=(20,20), \n",
    "        cmap=\"magma\", \n",
    "        patch_artist=True)\n",
    "plt.tight_layout();\n",
    "\n",
    "# classification problemlerinde outlier tespiti yaparken class bazında bakmazsak hata yapabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "for idx, col in enumerate(df.select_dtypes(include='number').columns[:-1]):\n",
    "    plt.subplot(5, 6, idx+1)\n",
    "    sns.histplot(data=df, x=col, hue='Cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__  # numeric_only=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "id": "fNDK3HprhSU3",
    "outputId": "98fe8eba-dc36-4cc7-96ad-ff61182d12a2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 11))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, vmin=-1, vmax=1, cmap=\"coolwarm\", cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6M897UMiREU"
   },
   "outputs": [],
   "source": [
    "drop_list = [\"worst perimeter\", \"worst area\", \"perimeter error\", \"area error\", \"mean perimeter\", \"mean area\"]\n",
    "# yüksek korelasyonlu featureları düşürüyorum modelimin hızlı çalışması için ama domain knowledge'ı olan birine ulaşabiliyorsam\n",
    "# ona danışarak bu işlemi yapmalıyız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(drop_list, axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 9))\n",
    "ax = df1.corr()[\"Cancer\"].sort_values().drop(\"Cancer\").plot(kind=\"barh\")\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.4f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoxdepUb9YfL"
   },
   "source": [
    "## Preprocessing of Data\n",
    "- Train | Test Split, Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjY-ZSPWka7D"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjyTH753lq7M"
   },
   "outputs": [],
   "source": [
    "X = df1.drop('Cancer', axis=1)\n",
    "y = df1['Cancer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,stratify=y, random_state=seed)\n",
    "                 ...\n",
    "    \n",
    "                 ...\n",
    "        \n",
    "                 ...\n",
    "model.fit(x = X_train, y = y_train, validation_split = 0.10, batch_size = 128, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classification problemlerinde özellikle imbalanced datalarda validation_split kullanmak bizim kötü skorlar almamıza neden\n",
    "# olabilir . Bundan dolayı validation_data belirlememiz gerekiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test, y_train1, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.1, random_state=seed \n",
    ")\n",
    "\n",
    "# classların dağılımının oranını göz önünde bulundurarak bu train datasını ve test datasını ayırdık.\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train1, y_train1, stratify=y_train1, test_size=0.1, random_state=seed \n",
    ")\n",
    "\n",
    "# classların dağılımının oranını göz önünde bulundurarak bu train datasını ve validation datasını ayırdık."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTcCuQAm9YfM"
   },
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "scaler = RobustScaler()\n",
    "# ann arka planda gradient descent çalıştırdığı için scaler'a ihtiyaç duyar.\n",
    "## bütün scaling çeşitlerini deneyebiliriz RobustScaler() daha iyi skorlar verdiğinden onunla devam ediyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fgy3D9SBmWcq"
   },
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha9hDt_AoGaE"
   },
   "source": [
    "## Modelling & Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzzxpfNaoFyZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53becHwOn549",
    "outputId": "2e558a0d-f10e-426f-ae9b-0937c35ccdd7"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TX7hyUw5rOZc",
    "outputId": "90d240eb-1271-4374-ab8f-eb8fcc0f0eed"
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GG2glFRNrQcv"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "# biz normalde modelin mimarisini her kurduğumuzda random olarak ağırlıkları ve bias'leri atıyor ama biz şimdi \n",
    "# bazı kavramlar göreceğiz onların farklarını görebilmek için \n",
    "# tf.keras.utils.set_random_seed(seed)  ile model her çalıştığında aynı ağırlıkları atasın istiyorum.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(36, activation = \"relu\", input_dim = X_train.shape[1]))\n",
    "model.add(Dense(18, activation = \"relu\"))\n",
    "model.add(Dense(9, activation = \"relu\"))\n",
    "#model.add(Activation(\"relu\")) # Activation function can be added separately as a different line after each layer. \n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "# targetımız binary bir yapıda olduğundan  output layerda 1  nöron olacak şekilde mimamirizi oluşturuyoruz.\n",
    "# activation = \"sigmoid\" yazdığımıza dikkat edelim hiçbir şey yazmazsak defatulu linear'di hatırlarsak.\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "# problemimiz classification problemi olduğundan loss = \"binary_crossentropy\" yapıyoruz .\n",
    "# optimizer hyperparametresi ağırlıkları güncellerken ; \n",
    "# hangi gradient descent türünü kullanmak istediğimizle alakalıdır 'rmsprop' gibi \"adam\" gibi optimizer çeşitlerini deneyerek\n",
    "# en iyi sonucu hangisi veriyorsa yola onunla devam etmemiz gerekir burada optimizer türü datadan dataya göre farklılık gösterir\n",
    "#metrics=[\"accuracy\"] ile, modelin accuracy'sinin de takip edilebileceğini belirtiyoruz.\n",
    "# Bu, eğitim sırasında modelin accuracy'sini de izlememize olanak sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biz bütün classları %100 başarıyla tahmin edersek loss = binary_crossentropy sıfır çıkar teoride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hw5J9o-mtbkj",
    "outputId": "0b760bf3-0ef2-4afa-c80d-b81d9675b07b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          batch_size=32, \n",
    "          epochs=600,\n",
    "          verbose=1)\n",
    "\n",
    "\n",
    "#  validation_data=(X_val, y_val) yukarıda ayırdığımız validation datasını kullanıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQIbjGQduQpQ",
    "outputId": "f94d855f-ff16-40a1-fd7a-be5a9bdfea0f"
   },
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "PQrRXkriuaux",
    "outputId": "d88dc909-3548-42a5-c224-1d2c9aed079e"
   },
   "outputs": [],
   "source": [
    "loss_df.plot(subplots=[[\"loss\",\"val_loss\"],[\"accuracy\",\"val_accuracy\"]], layout=(2,1),figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelimizin overfittinge gittiği çok ragat bir şekilde gözüküyor loss val_loss'u karşılaştırdığımızda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NhLecKb9YfP",
    "outputId": "521ad272-c86d-4346-9dca-c6d046442fc2"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aP2E3pMe9YfP",
    "outputId": "b3d56573-20e5-4045-ffed-29e3ab255a2e"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"loss : \", loss)\n",
    "print(\"accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-6BflwF9YfP",
    "outputId": "00689d20-57db-4d09-bdd9-7e259de60703"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) > 0.5\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# burada predict doğrudan çalışmıyor bize olasılıklar döndürdüğü için 0.5'den yukarıda olanları 1 classına ataması için\n",
    "# model.predict(X_test) > 0.5 şeklinde kullanıyoruz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modelimizin test datasında skorları yüksek ama grafikte de gördük modelimiz overfittinge gitmiş bizim bu modeli overfitinden \n",
    "## kurtarmamız gerekecek.Çünkü genelleme yapamıyor şu anda ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UogGQGs4vsuD"
   },
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D17nNk7vrDT"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcPSEww_u_wp"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(36, activation = \"relu\", input_dim = X_train.shape[1]))\n",
    "model.add(Dense(18, activation = \"relu\"))\n",
    "model.add(Dense(9, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gT47GwcwC2Q"
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", verbose = 1, patience = 15, restore_best_weights = True)\n",
    "\n",
    "\n",
    "\n",
    "# \"monitor\" : Takip edilecek skoru yazıyoruz buraya mesela val_loss kullandık ,val_accuracy'de yazabilirdik\n",
    "# biz val_loss'un minimum olmasını istediğimiz için mode = \"min\" yaptık .val_accuracy yazsaydık mode = \"max\" yapacaktık\n",
    "# patience ise 25 epoch boyunca eğer val_loss düşmezse eğitimi durdurmasını söylüyoruz patience olarak da best pratice şu \n",
    "# olur diyebiliceğimiz bir durum yoktur .ama genelde 10-25 arası bir değer kullanılır.\n",
    "#m odel eğer 25 epoch boyunca val_loss değerinde bir iyileşme görmezse eğitimi keser.\n",
    "# restore_best_weights = True ise modelin eğitimi kestiği yerdeki değil de 25 epoch önceki ağırlıklar ile modeli kurmasını\n",
    "# istediğimizi söylüyoruz default'u False'tur ve öyle bırakırsak eğitimi kestiği yerdeki ağırlıklar ile model eğitilmiş olur ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6h5PWZqwHq0",
    "outputId": "3aff7f04-b48d-4b43-94ed-c51528ddb538"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=600,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop],\n",
    ")\n",
    "\n",
    "\n",
    "## yukarıda tanımladığımız early stop değişkenini model.fit içerisinde callbacks hyperparametresine LİSTE içerisinde veriyoruz\n",
    "# ki model early_stop kullanacağını anlasın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "_y5lzpn-xKkj",
    "outputId": "3609ecfe-1c67-42d2-b308-850b014969bc"
   },
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot(subplots=[[\"loss\",\"val_loss\"],[\"accuracy\",\"val_accuracy\"]], layout=(2,1),figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPyyh3yS9YfT",
    "outputId": "d51de804-94b1-4844-c522-de00483e5857"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"loss : \", loss)\n",
    "print(\"accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIoHvzgU9YfT",
    "outputId": "5e5ffb04-5b81-4abc-c81b-aeed44b9bc46"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) > 0.5\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model_cancer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----> hata sayısı ve skorlar aynı olabilir ama şu anda elimde daha güvenilir bir model var overfittingten kurtardık."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DepRGvPm9YfU"
   },
   "source": [
    "### learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKuAH4a29YfU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "## biz eğer bir optimizer'ın default learnig rate'ini değiştirmek istiyorsak tensorflow.keras.optimizers'dan\n",
    "## hangi optimizerın learning rate'ini değiştireceksek önce onu import ediyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imohuWuc9YfU"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(36, activation = \"relu\", input_dim = X_train.shape[1]))\n",
    "model.add(Dense(18, activation = \"relu\"))\n",
    "model.add(Dense(9, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "opt = Adam(learning_rate=  0.005)\n",
    "\n",
    "# optimizer'ı compile'a string olarak verdiğimizde \"adam\" gibi ; bu default learning rate kullanacağım demektir bunu değiştirmek\n",
    "# istiyorsak opt = Adam(learning_rate=  0.005) bu şekilde tanımlayıp içerisine istediğimiz lr oranını yazıp\n",
    "# ardından compile satırına  bunu tanımlıyoruz :\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXaAqrut9YfV"
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", verbose = 1, patience = 15, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvRig35r9YfV",
    "outputId": "d80bd2b6-177b-48fa-ada9-616bc696e7e7"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=600,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXV32g-59YfV",
    "outputId": "835fb851-b949-4ec2-ed8f-c024cb3404c9"
   },
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot(subplots=[[\"loss\",\"val_loss\"],[\"accuracy\",\"val_accuracy\"]], layout=(2,1),figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvTUGw739YfW",
    "outputId": "2a2bbe1b-1b96-4a6c-9737-fbfd75e6e3d5"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"loss : \", loss)\n",
    "print(\"accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXjDG1Xk9YfW",
    "outputId": "0bdf467b-0559-4081-b18f-c979e3d8b915"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) > 0.5\n",
    "#y_pred = model.predict_classes(X_test) for tf 2.5.0\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## daha kötü bir lokal minimuma girdi ve overfittinge meyilli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiIoSQ2yxiB"
   },
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lowha5uv9YfX"
   },
   "source": [
    "The Dropout layer randomly sets input units to 0 with a frequency of `rate`\n",
    "at each step during training time, which helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRIThyJUyweN"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# ---> Dropout'ta bir regularization tekniğidir.\n",
    "# Dropout, ağın belirli bir kısmını her İTERASYONDA rastgele seçerek \"eğitim\" sırasında devre dışı bırakır.Yukarıda şekilde \n",
    "# görüleceği gibi. biz modelimizin datayı ezberlemesini engellemek isteriz datadaki kuralları öğrenmesini datanın yapısını \n",
    "# anlamasını isteriz ama model datayı takıntı haline getirirse yani ezberlerse bu dropout yöntemi de kullanabileceğimiz bir \n",
    "# regularization tekniğidir.\n",
    "\n",
    "\n",
    "# Dropout'u bir layer gibi modelimize ekleyeceğiz aşağıdaki örnekte olduğu gibi. \n",
    "# model.add(Dropout(0.2)) demek her iterasyonda nöronların MAX  %20 'si kapanacak demektir ve bu her iterasyonda RASTGELE olur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJokLHStyr1h"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(36, activation=\"relu\", input_dim = X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(18, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(9, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = Adam(learning_rate = 0.001)\n",
    "model.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeQ4lysU0XxP",
    "outputId": "c26f2bb6-5946-4ef4-a450-4718ab451bc0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=600,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15NoXUmN9YfY",
    "outputId": "b7dd943e-6400-460e-eb67-705247714a83"
   },
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot(subplots=[[\"loss\",\"val_loss\"],[\"accuracy\",\"val_accuracy\"]], layout=(2,1),figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1dxXOeL9YfY",
    "outputId": "94a73fa2-f0ed-48a2-988d-6519d1af6376"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"loss : \", loss)\n",
    "print(\"accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) > 0.5\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----> dropout ile de overfittingi giderebildik.\n",
    "# ! Hem dropout hem earlystop aynı anda kullanılabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularizations(Weight Decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Why should we use kernel_regularizer?***\n",
    "\n",
    "- Kernel_regularizer is a technique that reduces the amount of weight of a network by allocating large weights to smaller ones. When kernel_regularizer is applied, the weights become smaller and the network is less likely to overfit.\n",
    "\n",
    "Regularizers allow you to apply penalties on layer parameters or layer activity during optimization. These penalties are summed into the loss function that the network optimizes.\n",
    "\n",
    "Regularization penalties are applied on a per-layer basis. The exact API will depend on the layer, but many layers (e.g. Dense, Conv1D, Conv2D and Conv3D) have a unified API.\n",
    "\n",
    "***These layers expose 3 keyword arguments:***\n",
    "\n",
    "- kernel_regularizer: Regularizer to apply a penalty on the layer's kernel\n",
    "- bias_regularizer: Regularizer to apply a penalty on the layer's bias\n",
    "- activity_regularizer: Regularizer to apply a penalty on the layer's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1, l2, L1L2\n",
    "\n",
    "## deep learningte kernel'e yani weightlere , aktivasyon fonksiyonlarına ve bias'lere doğrudan penalty eklenebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(36, activation=\"relu\", \n",
    "                kernel_regularizer=L1L2(l1=1e-3, l2=1e-3),\n",
    "                bias_regularizer=l2(1e-4), \n",
    "                activity_regularizer=l1(1e-5), \n",
    "                input_dim=X_train.shape[1]))\n",
    "\n",
    "model.add(Dense(18, activation=\"relu\", \n",
    "                kernel_regularizer=L1L2(l1=1e-3, l2=1e-3), \n",
    "                bias_regularizer=l2(1e-4), \n",
    "                activity_regularizer=l1(1e-5)))\n",
    "\n",
    "model.add(Dense(9, activation=\"relu\", \n",
    "                kernel_regularizer=L1L2(l1=1e-3, l2=1e-3), \n",
    "                bias_regularizer=l2(1e-4), \n",
    "                activity_regularizer=l1(1e-5)))\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train,\n",
    "          y=y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          batch_size=32, \n",
    "          epochs=600,\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot(subplots=[[\"loss\",\"val_loss\"], [\"accuracy\",\"val_accuracy\"]], layout=(2,1), figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) > 0.5\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## skorlar daha da iyileşti overfittingte yok şimdiye kadar ki en iyi model bu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"l1l2_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjppNAhw9YfZ"
   },
   "source": [
    "## Cross Validation\n",
    "\n",
    "**Keras models** can be used in **scikit-learn** by wrapping them with the **KerasClassifier** or **KerasRegressor** class.\n",
    "\n",
    "To use these wrappers you must define a function that creates and returns your Keras sequential model, then pass this function to the build_fn argument when constructing the KerasClassifier class.\n",
    "\n",
    "The constructor for the KerasClassifier class can take default arguments that are passed on to the calls to model.fit(), such as the number of epochs and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wm_8MCu9Yfa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "## cross_validate sklearnde olan bir uygulama biz doğrudan cross_validate'in içerisine ann ile kurduğumuz modeli verirsek hata\n",
    "#alırız bunun için tensorflow.keras.wrappers.scikit_learn import KerasClassifier ile modelimizin üzerine adeta bir scikit_learn\n",
    "# ambalajı geçirerek cross_validation yapabilme imkanı elde ediyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4W-EiUJY9Yfa"
   },
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 36, activation = 'relu'))\n",
    "    classifier.add(Dense(units = 18, activation = 'relu'))\n",
    "    classifier.add(Dense(units = 9, activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9gTVfw89Yfa",
    "outputId": "7ae992e6-2318-47ca-970e-6fce7d832ad0"
   },
   "outputs": [],
   "source": [
    "classifier_model = KerasClassifier(build_fn = build_classifier, batch_size = 32, epochs = 100, verbose=0)\n",
    "\n",
    "# classifier_model'i sklearndeki bir classifaciton algoritması olarak kullanabiliriz artık !\n",
    "\n",
    "scores = cross_validate(estimator = classifier_model, X = X_train, y = y_train,\n",
    "                        scoring = ['accuracy', 'precision', 'recall', 'f1'], cv = 10)\n",
    "\n",
    "# \n",
    "\n",
    "df_scores = pd.DataFrame(scores, index = range(1, 11)).iloc[:, 2:]\n",
    "\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JutFQ679Yfa",
    "outputId": "8b10b5ac-852e-492e-ad91-b0b27b68711a"
   },
   "outputs": [],
   "source": [
    "df_scores_summary = pd.DataFrame({\"score_mean\" : df_scores.mean().values, \"score_std\" : df_scores.std().values},\n",
    "                                 index = [\"acc\", \"pre\", \"rec\", \"f1\"])\n",
    "\n",
    "df_scores_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----> cv teorik olarak uygulanabilen ama pratikte uygulanmayan bir durumdur ; modelimizi her kurduğumuzda farklı ağırlıklarla\n",
    "## başlar çünkü bu cv'ının mantığına aykırı bir durumdur burada 10 farklı model deniyoruz! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "https://optuna.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, Adadelta, RMSprop, Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "## GridSearch'i de aynı cv'da olduğu gibi kullanabilriz burada ama optunayı kullanacağız bunun nedenleri\n",
    "## ---> sklearn GPU'yu kullanamaz , gridsearch verdiğimiz uzaydaki bütün kombinasyonları deniyor DL'de çok fazla hyperparametre\n",
    "# var bunun için GPU'yu kullanabileceğimiz bir optimization yöntemi olan optuna'yı kullanıyoruz.\n",
    "\n",
    "# optunayı Ml algoritmalarında da kullanabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\", mode=\"min\", verbose=1, patience=15, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_metric = \"Accuracy\"\n",
    "batch_size = 64\n",
    "\n",
    "# batch_size = 64 model biraz daha hızlı eğitilsin diye batch_size'ı 32'den 64'e çıkardık\n",
    "\n",
    "def create_model(trial):\n",
    "    # Some hyperparameters we want to optimize\n",
    "    n_units1 = trial.suggest_int(\"n_units1\", 64, 128) # ilk layerda kaç tane nöron olsun 64'ten 128'e kadar dene diyoruz\n",
    "    n_units2 = trial.suggest_int(\"n_units2\", 16, 64)  # ama dikkat hepsini denemeyecek en iyiye doğru bir yönelimi olacak. \n",
    "    n_units3 = trial.suggest_int(\"n_units3\", 16, 64)\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", [Adam, Adadelta, RMSprop, Nadam]) # optimizer olarak buradakileri dene\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1) \n",
    "    regularizer1 = trial.suggest_loguniform(\"regularizer1\", 1e-5, 1e-3)\n",
    "    regularizer2 = trial.suggest_loguniform(\"regularizer2\", 1e-5, 1e-3)\n",
    "    dropout = trial.suggest_categorical(\"dropout\",[0, .3, .5])\n",
    "    \n",
    "    # deneyeceğimiz hyperparametrelerin uzayını belirledik.\n",
    "    # modeli aynı seed'de çalıştırarak\n",
    "\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_units1,  \n",
    "                    activation=\"relu\", \n",
    "                    kernel_regularizer=L1L2(l1=regularizer1, l2=regularizer2), \n",
    "                    bias_regularizer=l2(regularizer2), \n",
    "                    activity_regularizer=l1(regularizer2), \n",
    "                    input_dim=X_train.shape[1],))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_units2, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_regularizer=L1L2(l1=regularizer1, l2=regularizer2), \n",
    "                    bias_regularizer=l2(regularizer2), \n",
    "                    activity_regularizer=l1(regularizer2)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_units3,\n",
    "                    activation=\"relu\", \n",
    "                    kernel_regularizer=L1L2(l1=regularizer1, l2=regularizer2),\n",
    "                    bias_regularizer=l2(regularizer2), \n",
    "                    activity_regularizer=l1(regularizer2)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=optimizer(learning_rate=learning_rate),\n",
    "        metrics=[trial_metric]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    w0 = trial.suggest_loguniform(\"w0\", 0.01, 5) # mimarinin içerisinde direk veremediğimiz bir hyperparametre-->(class_weight)\n",
    "    w1 = trial.suggest_loguniform(\"w1\", 0.01, 2) # daha var bunu fit içerisinde belirtebiliyoruz ondan dolayı burada tanımladık\n",
    "                                                 # denenmesi için.\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=700,\n",
    "        callbacks=[early_stop],\n",
    "        class_weight={0: w0, 1: w1},\n",
    "        verbose=0,\n",
    "    )\n",
    "    return model.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\") # trial_metric = \"Accuracy\" olduğundan bunu maximize etmek istiyoruz.\n",
    "study.optimize(objective, n_trials=15) # 15 trials denenecek istersek daha fazla da yazabiliriz.optimize edeceği :def objective\n",
    "display(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## her çalıştırdığımızda farklı şeyler deneyecek ; seed olsa da(başlangıçta aynı yerden başlasa da )\n",
    "# kendisi farklı şeyler deneyerek  devam ediyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model with optuna parameters\n",
    "unit1, unit2, unit3, optimizer, lr, lreg1, lreg2, dropout_rate, w0, w1 = (\n",
    "    study.best_params[\"n_units1\"],\n",
    "    study.best_params[\"n_units2\"],\n",
    "    study.best_params[\"n_units3\"],\n",
    "    study.best_params[\"optimizer\"],\n",
    "    study.best_params[\"learning_rate\"],\n",
    "    study.best_params[\"regularizer1\"],\n",
    "    study.best_params[\"regularizer2\"],\n",
    "    study.best_params[\"dropout\"],\n",
    "    study.best_params[\"w0\"],\n",
    "    study.best_params[\"w1\"],\n",
    ")\n",
    "\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(unit1, \n",
    "                activation=\"relu\", \n",
    "                kernel_regularizer=L1L2(l1=lreg1, l2=lreg2), \n",
    "                bias_regularizer=l2(lreg2), \n",
    "                activity_regularizer=l2(lreg2)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(unit2, \n",
    "                activation=\"relu\", \n",
    "                kernel_regularizer=L1L2(l1=lreg1, l2=lreg2), \n",
    "                bias_regularizer=l2(lreg2), \n",
    "                activity_regularizer=l2(lreg2)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(unit3, \n",
    "                activation=\"relu\", \n",
    "                kernel_regularizer=L1L2(l1=lreg1, l2=lreg2), \n",
    "                bias_regularizer=l2(lreg2), \n",
    "                activity_regularizer=l2(lreg2)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "opt = optimizer(learning_rate=lr)\n",
    "model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"Accuracy\"])\n",
    "\n",
    "# train model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=64,\n",
    "    epochs=500,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(history)\n",
    "loss_df.plot(subplots=[[\"loss\",\"val_loss\"],[\"Accuracy\",\"val_Accuracy\"]], layout=(2,1),figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bizim kurduğum l1l2 model optunanın bize verdğinden daha iyi onunla devam edebiliriz .UNUTMAYALIM :\n",
    "# optuna bize bir başlangıç noktası verir buradan gelecek hyperparametreler mutlak doğrudur gibi bir yaklaşım olmamalıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC (Receiver Operating Curve) and AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX4AtBkk9Yff"
   },
   "source": [
    "## Saving Final Model and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAy2Fcaa9Yff"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(scaler, open(\"scaler_cancer\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ufcDdZ39Yfg",
    "outputId": "ecbc6eda-9482-4158-ad69-429edefb1f52"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) > 0.5\n",
    "#y_pred = model.predict_classes(X_test) for tf 2.5.0\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_cancer.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tfKL5Db9Yfg"
   },
   "source": [
    "## Loading Model and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AtX6bkW9Yfg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-anTS6tk9Yfg"
   },
   "outputs": [],
   "source": [
    "model_cancer = load_model('model_cancer.h5')\n",
    "#model_cancer = load_model(\"l1l2_model.h5\")\n",
    "scaler_cancer = pickle.load(open(\"scaler_cancer\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq10ovAX6daY"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdWf-EXe9Yfh",
    "outputId": "71c013f2-38ce-432b-ccfa-c6e26e5575aa"
   },
   "outputs": [],
   "source": [
    "single_patient = df1.drop('Cancer', axis = 1).iloc[0:1, :]\n",
    "single_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xiIxkAl9Yfh",
    "outputId": "ef7759d7-a71e-44a1-96bf-eb29da93a205"
   },
   "outputs": [],
   "source": [
    "single_patient = scaler_cancer.transform(single_patient)\n",
    "single_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbbjwAhZ9Yfh",
    "outputId": "949ea52e-7b9d-4e11-9cee-aac363f8eead"
   },
   "outputs": [],
   "source": [
    "(model_cancer.predict(single_patient) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWQWw8-F9Yfh",
    "outputId": "6fdaa29a-cec2-4e8c-8b55-2d7a16debe78"
   },
   "outputs": [],
   "source": [
    "df[\"Cancer\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfUzOmSq9Yfh"
   },
   "source": [
    "## Comparison with ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, stratify=y, test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgaUsf0w9Yfh"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFAPPGYm9Yfi"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCvoWhUl9Yfi",
    "outputId": "e78281b2-00ee-4698-da61-5e9059f37ce1"
   },
   "outputs": [],
   "source": [
    "log_model=LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred = log_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YDKRcVc9Yfi"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMk1gh149Yfi"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryH-lANR9Yfi",
    "outputId": "bec225b7-2d54-4ba6-d983-90044e0228df"
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P4HpdiO9Yfi"
   },
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
